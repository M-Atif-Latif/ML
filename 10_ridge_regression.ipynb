{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4de202",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression where a regularization term equal to λ∑θj² is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure.\n",
    "\n",
    "## What is Regularization?\n",
    "\n",
    "Regularization is a technique used in machine learning and statistics to prevent overfitting of models on training data. Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization to new, unseen data. Regularization helps to solve this problem by adding a penalty to the model's complexity.\n",
    "\n",
    "## Ridge Regression Explained\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term. The key idea behind ridge regression is to find a new line that doesn't fit the training data as well as ordinary least squares regression, in order to achieve better generalization to new data. This is particularly useful when:\n",
    "\n",
    "- Dealing with multicollinearity (independent variables are highly correlated)\n",
    "- The number of predictors (features) exceeds the number of observations\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "**Regularization**: Ridge regression adds a penalty equal to the square of the magnitude of coefficients. This penalty term (squared L2 norm) shrinks the coefficients towards zero, but it doesn't make them exactly zero.\n",
    "\n",
    "## Mathematical Representation\n",
    "\n",
    "The ridge regression modifies the least squares objective function by adding a penalty term:\n",
    "\n",
    "$$\\sum_{i=1}^{n}(y_i - \\sum_{j=1}^{p} x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y_i$ is the response value for the $i$th observation.\n",
    "- $x_{ij}$ is the value of the $j$th predictor for the $i$th observation.\n",
    "- $\\beta_j$ is the regression coefficient for the $j$th predictor.\n",
    "- $\\lambda$ is the tuning parameter that controls the strength of the penalty; $\\lambda \\geq 0$.\n",
    "\n",
    "In code implementations, `alpha` is the regularization strength ($\\lambda$). Adjusting alpha changes the strength of the regularization penalty. A larger alpha enforces stronger regularization (leading to smaller coefficients), and a smaller alpha tends towards a model similar to linear regression.\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Choosing Alpha**: Selecting the right value of alpha is crucial. It can be done using cross-validation techniques like RidgeCV.\n",
    "- **Standardization**: It's often recommended to standardize the predictors before applying ridge regression.\n",
    "- **Bias-Variance Tradeoff**: Ridge regression balances the bias-variance tradeoff in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2b02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1. 2.]\n",
      "Intercept: 3.0000000000000018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# Target values\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", lr.coef_)\n",
    "# Intercept\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de35073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.90909091 1.63636364]\n",
      "Intercept: 3.8636363636363633\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# Target values\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Ridge Regression Model\n",
    "ridge_reg = Ridge(alpha=0.5)  # alpha is the equivalent of lambda in the formula\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", ridge_reg.coef_)\n",
    "# Intercept\n",
    "print(\"Intercept:\", ridge_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459631e",
   "metadata": {},
   "source": [
    "## Comparing Simple Linear Regression vs. Ridge Regression\n",
    "### Import Libraries and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee24aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the data set\n",
    "df = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f3432",
   "metadata": {},
   "source": [
    "### Pre Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c4f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features X and the target/labels y\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888325d1",
   "metadata": {},
   "source": [
    "### Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da47e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Pipeline\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('regressor', LinearRegression())])\n",
    "\n",
    "# Ridge Regression Pipeline\n",
    "ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('regressor', Ridge(alpha=0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b7bea",
   "metadata": {},
   "source": [
    "### train and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd1ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 1288705.4778516763\n",
      "Ridge Regression MSE: 1288691.2489788432\n",
      "------------------------\n",
      "Linear Regression R2: 0.9189331350419386\n",
      "Ridge Regression R2: 0.9189340301185347\n",
      "------------------------\n",
      "Linear Regression MAE: 737.1513665933285\n",
      "Ridge Regression MAE: 737.1455056792028\n",
      "------------------------\n",
      "Linear Regression MAPE: 0.3952933516494362\n",
      "Ridge Regression MAPE: 0.3952510372855173\n",
      "------------------------\n",
      "Linear Regression RMSE: 1135.2116445190634\n",
      "Ridge Regression RMSE: 1135.2053774444707\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Linear Regression\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_pred = lr_pipeline.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "lr_mape = mean_absolute_percentage_error(y_test, lr_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "\n",
    "# Train and evaluate Ridge Regression\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "ridge_pred = ridge_pipeline.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "ridhe_r2 = r2_score(y_test, ridge_pred)\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_pred)\n",
    "ridge_mape = mean_absolute_percentage_error(y_test, ridge_pred)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "\n",
    "print(\"Linear Regression MSE:\", lr_mse)\n",
    "print(\"Ridge Regression MSE:\", ridge_mse)\n",
    "print(f\"------------------------\")\n",
    "\n",
    "print(\"Linear Regression R2:\", lr_r2)\n",
    "print(\"Ridge Regression R2:\", ridhe_r2)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression MAE:\", lr_mae)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression MAPE:\", lr_mape)\n",
    "print(\"Ridge Regression MAPE:\", ridge_mape)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a19927",
   "metadata": {},
   "source": [
    "## Summary of the notebook:\n",
    "### 1. Introduced Ridge Regression and regularization concepts.\n",
    "### 2. Demonstrated basic Linear and Ridge Regression with small numpy arrays.\n",
    "### 3. Loaded the diamonds dataset and split it into features (X) and target (y).\n",
    "### 4. Identified numeric and categorical features for preprocessing.\n",
    "### 5. Built a preprocessing pipeline using StandardScaler for numeric and OneHotEncoder for categorical features.\n",
    "### 6. Split the data into training and test sets.\n",
    "### 7. Created pipelines for both Linear Regression and Ridge Regression models.\n",
    "### 8. Trained and evaluated both models, comparing metrics such as MSE, R2, MAE, MAPE, and RMSE on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c99938",
   "metadata": {},
   "source": [
    "# About the Author\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border-left: 5px solid #28a745; padding: 20px; margin-bottom: 20px; border-radius: 5px;\">\n",
    "  <h2 style=\"color: #28a745; margin-top: 0; font-family: 'Poppins', sans-serif;\">Muhammad Atif Latif</h2>\n",
    "  <p style=\"font-size: 16px; color: #495057;\">Data Scientist & Machine Learning Engineer</p>\n",
    "  \n",
    "  <p style=\"font-size: 15px; color: #6c757d; margin-top: 15px;\">\n",
    "    Passionate about building AI solutions that solve real-world problems. Specialized in machine learning, \n",
    "    deep learning, and data analytics with experience implementing production-ready models.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "## Connect With Me\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;\">\n",
    "  <a href=\"https://github.com/m-Atif-Latif\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/GitHub-Follow-212121?style=for-the-badge&logo=github\" alt=\"GitHub\">\n",
    "  </a>\n",
    "  <a href=\"https://www.kaggle.com/matiflatif\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\">\n",
    "  </a>\n",
    "  <a href=\"https://www.linkedin.com/in/muhammad-atif-latif-13a171318\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\">\n",
    "  </a>\n",
    "  <a href=\"https://x.com/mianatif5867\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2?style=for-the-badge&logo=twitter\" alt=\"Twitter\">\n",
    "  </a>\n",
    "  <a href=\"https://www.instagram.com/its_atif_ai/\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Instagram-Follow-E4405F?style=for-the-badge&logo=instagram\" alt=\"Instagram\">\n",
    "  </a>\n",
    "  <a href=\"mailto:muhammadatiflatif67@gmail.com\">\n",
    "    <img src=\"https://img.shields.io/badge/Email-Contact-D14836?style=for-the-badge&logo=gmail\" alt=\"Email\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAchine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
