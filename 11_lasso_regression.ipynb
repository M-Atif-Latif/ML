{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17786d4",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Lasso Regression, which stands for **Least Absolute Shrinkage and Selection Operator**, is a type of linear regression that uses shrinkage. Shrinkage here means that the data values are shrunk towards a central point, like the mean. The lasso technique encourages simple, sparse models (i.e., models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "## Key Features of Lasso Regression:\n",
    "\n",
    "* **Regularization Term**: The key characteristic of Lasso Regression is that it adds an L1 penalty to the regression model, which is the absolute value of the magnitude of the coefficients. The cost function for Lasso regression is:\n",
    "\n",
    "  $$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "  where $\\alpha$ is the regularization parameter.\n",
    "\n",
    "* **Feature Selection**: One of the advantages of lasso regression over ridge regression is that it can result in sparse models with few coefficients; some coefficients can become exactly zero and be eliminated from the model. This property is called automatic feature selection and is a form of embedded method.\n",
    "\n",
    "* **Parameter Tuning**: The strength of the L1 penalty is determined by a parameter, typically denoted as alpha or lambda. Selecting a good value for this parameter is crucial and is typically done using cross-validation.\n",
    "\n",
    "* **Bias-Variance Tradeoff**: Similar to ridge regression, lasso also manages the bias-variance tradeoff in model training. Increasing the regularization strength increases bias but decreases variance, potentially leading to better generalization on unseen data.\n",
    "\n",
    "* **Scaling**: Before applying lasso, it is recommended to scale/normalize the data as lasso is sensitive to the scale of input features.\n",
    "\n",
    "## Implementation in Scikit-Learn:\n",
    "\n",
    "Lasso regression can be implemented using the `Lasso` class from Scikit-Learn's `linear_model` module. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa15abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Linear Regression: 0.0111837651150914\n",
      "MSE of Lasso: 0.3847492638484584\n",
      "MSE of Ridge: 0.05090866185226863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=15, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=0.2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"MSE of Linear Regression:\", mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"MSE of Lasso:\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"MSE of Ridge:\", mean_squared_error(y_test, y_pred_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5bf6b",
   "metadata": {},
   "source": [
    "## Code Explanation - Linear, Lasso, and Ridge Regression Comparison\n",
    "\n",
    "In this code block, I:\n",
    "\n",
    "1. Imported necessary models (`Lasso`, `Ridge`, `LinearRegression`) and utility functions from scikit-learn\n",
    "2. Generated synthetic regression data with 1000 samples and 15 features\n",
    "3. Split the data into training (80%) and testing (20%) sets\n",
    "4. Created three regression models:\n",
    "   - Standard Linear Regression (no regularization)\n",
    "   - Lasso Regression (L1 regularization with alpha=0.2)\n",
    "   - Ridge Regression (L2 regularization with alpha=1.0)\n",
    "5. Trained all three models on the training data\n",
    "6. Made predictions on the test set with each model\n",
    "7. Calculated and printed the Mean Squared Error (MSE) to compare the performance of all three models\n",
    "\n",
    "This demonstrates how different regularization techniques affect model performance on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7880a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(1, 10, 0.1)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d2c50",
   "metadata": {},
   "source": [
    "## Code Explanation - Creating a Range\n",
    "\n",
    "In this code snippet, I:\n",
    "\n",
    "1. Imported the NumPy library\n",
    "2. Created an array `x` containing values from 1 to 10 with a step size of 0.1\n",
    "3. Checked the length of this array using `len(x)`\n",
    "\n",
    "This creates a sequence of values that can be used for parameter tuning in the next code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Lasso Regression Parameters: {'alpha': np.float64(1.0)}\n",
      "Best score is 0.9995685234915115\n",
      "Tuned Ridge Regression Parameters: {'alpha': np.float64(1.0)}\n",
      "Best score is 0.9999981195099323\n",
      "CPU times: total: 48.1 s\n",
      "Wall time: 49.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fine tune alpha value using cv\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lasso = Lasso()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.01)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=10, \n",
    "            # n_jobs=-2\n",
    "            )\n",
    "\n",
    "# Fit the model\n",
    "lasso_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Lasso Regression Parameters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Best score is {}\".format(lasso_cv.best_score_))\n",
    "\n",
    "# Create a Ridge regression object\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.01)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=10, \n",
    "                # n_jobs=-2\n",
    "                )\n",
    "\n",
    "# Fit the model\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Ridge Regression Parameters: {}\".format(ridge_cv.best_params_))\n",
    "print(\"Best score is {}\".format(ridge_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95d76f",
   "metadata": {},
   "source": [
    "## Code Explanation - Grid Search for Hyperparameter Tuning\n",
    "\n",
    "In this code block, I used Grid Search Cross-Validation to find the optimal alpha value for both Lasso and Ridge regression:\n",
    "\n",
    "1. Imported necessary tools for cross-validation and model selection\n",
    "2. Created Lasso and Ridge regression objects (without specifying alpha)\n",
    "3. Defined a parameter grid with alpha values ranging from 1 to 10 with 0.01 step size\n",
    "4. Set up GridSearchCV with 10-fold cross-validation for both models\n",
    "5. Trained both models using the entire dataset\n",
    "6. Printed the best alpha parameter and corresponding score for each model\n",
    "\n",
    "Grid Search exhaustively searches through all parameter combinations to find the optimal value. I used the `%%time` magic command to measure execution time since this is a computationally intensive process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1f1875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Lasso Regression Parameters: {'alpha': np.float64(1.02)}\n",
      "Best score is 0.9995510922555066\n",
      "Tuned Ridge Regression Parameters: {'alpha': np.float64(1.9600000000000009)}\n",
      "Best score is 0.9999939557904197\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fine tune alpha value using cv\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lasso = Lasso()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.01)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "lasso_cv = RandomizedSearchCV(lasso, param_grid, cv=10,\n",
    "            n_jobs=-2\n",
    "            )\n",
    "\n",
    "# Fit the model\n",
    "lasso_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Lasso Regression Parameters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Best score is {}\".format(lasso_cv.best_score_))\n",
    "\n",
    "# Create a Ridge regression object\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.01)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=10, \n",
    "                n_jobs=-2\n",
    "                )\n",
    "\n",
    "# Fit the model\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Ridge Regression Parameters: {}\".format(ridge_cv.best_params_))\n",
    "print(\"Best score is {}\".format(ridge_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8221ff3",
   "metadata": {},
   "source": [
    "## Code Explanation - Randomized Search for Faster Hyperparameter Tuning\n",
    "\n",
    "This code block performs the same hyperparameter tuning as the previous one, but using RandomizedSearchCV instead of GridSearchCV:\n",
    "\n",
    "1. Set up the same parameter grid for alpha values (1 to 10 with 0.01 step)\n",
    "2. Used RandomizedSearchCV instead of GridSearchCV to randomly sample from the parameter space\n",
    "3. Enabled parallel processing with `n_jobs=-2` (using all but one CPU core)\n",
    "4. Performed 10-fold cross-validation for both Lasso and Ridge regression\n",
    "5. Printed the best parameters and scores\n",
    "\n",
    "RandomizedSearchCV is more efficient than GridSearchCV as it doesn't test all parameter combinations, but rather samples a specified number of combinations. This is particularly useful for large parameter spaces, providing a good trade-off between computation time and optimization quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0d2c7",
   "metadata": {},
   "source": [
    "# About the Author\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border-left: 5px solid #28a745; padding: 20px; margin-bottom: 20px; border-radius: 5px;\">\n",
    "  <h2 style=\"color: #28a745; margin-top: 0; font-family: 'Poppins', sans-serif;\">Muhammad Atif Latif</h2>\n",
    "  <p style=\"font-size: 16px; color: #495057;\">Data Scientist & Machine Learning Engineer</p>\n",
    "  \n",
    "  <p style=\"font-size: 15px; color: #6c757d; margin-top: 15px;\">\n",
    "    Passionate about building AI solutions that solve real-world problems. Specialized in machine learning, \n",
    "    deep learning, and data analytics with experience implementing production-ready models.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "## Connect With Me\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;\">\n",
    "  <a href=\"https://github.com/m-Atif-Latif\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/GitHub-Follow-212121?style=for-the-badge&logo=github\" alt=\"GitHub\">\n",
    "  </a>\n",
    "  <a href=\"https://www.kaggle.com/matiflatif\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\">\n",
    "  </a>\n",
    "  <a href=\"https://www.linkedin.com/in/muhammad-atif-latif-13a171318\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\">\n",
    "  </a>\n",
    "  <a href=\"https://x.com/mianatif5867\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2?style=for-the-badge&logo=twitter\" alt=\"Twitter\">\n",
    "  </a>\n",
    "  <a href=\"https://www.instagram.com/its_atif_ai/\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/Instagram-Follow-E4405F?style=for-the-badge&logo=instagram\" alt=\"Instagram\">\n",
    "  </a>\n",
    "  <a href=\"mailto:muhammadatiflatif67@gmail.com\">\n",
    "    <img src=\"https://img.shields.io/badge/Email-Contact-D14836?style=for-the-badge&logo=gmail\" alt=\"Email\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
